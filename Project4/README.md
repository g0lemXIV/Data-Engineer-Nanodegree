## Project: Data Lake with Spark
![](https://lh3.googleusercontent.com/proxy/VReQrhnV5Lm0DNZ7pH2tT0leo7pjB2_CDLnw_IYEa8BA_CpAXobA_6yu3fH4CqIlP_g2GrHJJ1kIB_aOVYQ22J4658qiqvlk55P5kiuORCO1wYeSBSBM)
### Overview
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I am tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

I will be able to test database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

### Project Details  
**Datasets**
For this project, I'll be working with two datasets: ```song_data``` and ```log_data``` which reside in S3
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.  
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

Link for s3 bucket with datasets: [S3 bucket](https://s3.console.aws.amazon.com/s3/buckets/udacity-dend)

## State and justify database schema design and ETL pipeline  

The goal of this project is to facilitate the analysis of song data and customer logs. We choose Spark because it executes much faster by caching data in memory across multiple parallel operations. Apache Spark is a well-known technology and has support from companies around the globe. Web-based companies, like Chinese search engine Baidu, e-commerce operation Taobao, and social networking company Tencent, all run Spark-based operations at scale, with Tencentâ€™s 800 million active users reportedly generating over 700 TB of data per day for processing on a cluster of more than 8,000 compute nodes.

## ETL PIPELINE
We Ingest data from a database application to Lake Storage (in our case S3) than Load results into Spark Cluster, Process them and save results as parquet with optimal partition column. 

![ETL](https://i.ibb.co/9y6cVLm/sparkaws.jpg)
**Source**: udacity.com

## How to run
To properly create run the script You need to install spark locally or make spark EMR on AWS.  
**More info can be found here:** [Amazon EMR](https://aws.amazon.com/emr/getting-started/)
First You need to create access keys on Your AWS Account. next add credentials to dl.cfg as follow
```SQL
[ROLE]
AWS_ACCESS_KEY_ID=<your-access-id>
AWS_SECRET_ACCESS_KEY=<your-acces-key>
```
Make sure that input and output S3 bucket is correct (if You don't have s3 bucket for output files You need to create them)
```SQL
[DATA]
INPUT_PATH=s3a://udacity-dend
OUTPUT_PATH=s3a://<your-bucket>
SONG_DATA=song_data/*/*/*/*.json
LOG_DATA=log_data/*/*/*.json
```

## Run ETL
To run ETL pipeline run python script named `etl.py`
```
user:~/project_dir$:python etl.py
```

Then the end output should be:

```
user:~/project_dir$:Read song table to DataFrame from s3a://<your-bucket>/songs.parquet                                  
user:~/project_dir$:Write time table to parquet into s3a://<your-bucket>/songplays.parquet
```

You should see also reduced size (because of duplicated rows) and saving progress bar.

## Examples

After load all tables You can test etl with test.ipynb (as below)

```Python
df_song = spark.read.parquet(songs_path)
df_song.printSchema()
df_song.toPandas().head()
```
**output:**  

![spark test 1](https://i.ibb.co/TB4rqx1/sparktest-notebook.png)

 ```Python
df_song.createOrReplaceTempView("song")
spark.sql("""
    SELECT year, count(song_id) as num_songs
    FROM song
    GROUP bY year
    ORDER BY num_songs DESC
""").show(3)
```  
**output:**  

| Index        | year           | num_songs  |
| ------------- |:-------------:| -----:|
| 0    | 0 | 9 |
| 1     | 2004      |   3 |
| 2 | 2007      |    1 |






